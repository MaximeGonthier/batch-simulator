\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx, amssymb, amsfonts, color, todonotes, diagbox, colortbl, pdfpages, listings, amsmath, caption, subcaption, xspace, xcolor, pifont, fullpage, algorithm, algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\setlength{\parskip}{0.2 cm}

% Commands
\newcommand{\Node}[1]{\ensuremath{\mathrm{Node}_{#1}}\xspace}
\newcommand{\inputs}{\ensuremath{\mathcal{F}}\xspace}
\newcommand{\size}{\ensuremath{\mathcal{M}}\xspace}
\newcommand{\fileset}{\ensuremath{\mathbb{F}}\xspace}
\newcommand{\jobset}{\ensuremath{\mathbb{J}}\xspace}

\title{Data Aware Batch Scheduling}
\author{Maxime GONTHIER - Carl NETTELBLAD - Elisabeth LARSSON \\ Samuel THIBAULT - Loris MARCHAL}
\usepackage[backend=bibtex]{biblatex}
\bibliography{ref}

\begin{document}

\maketitle
\tableofcontents
\listoffigures
\newpage

\todo[inline]{Maxime: You can add highlighted comments this way: "\textbackslash todo[inline]\{Text\}".}
\todo{Maxime: Or with: "\textbackslash todo\\\{Text\}".}

\section{Motivation}

When using a HPC cluster, users may submits tens to hundreds jobs using the same multi-GB input file.
Users tend to submit each such run as a separate batch job.
The job will read inputs directly from a shared filesystem, which can be disastrous if the number of I/O are important.
\textbf{How can we minimize the amount of transfers between the shared filesystem and the workers ?}

Users can manually group together input files into a single job to reduce the effects from I/O reads.
Ideally, we would like to let the user submit jobs the way they want and schedule efficiently jobs on nodes.
The workers have a way to store files (with a limit on the memory size), as well as a cache page which contains the files/data recently used.
Thus, a way to minimize data transfers is to create a scheduler that \textbf{take into consideration data locality and schedule on the same nodes jobs sharing inputs}.

Our scheduler would be used to \textbf{distribute the load between the workers but also to have a vision of what the memory of each worker contains in order to re-use as much as possible a file already loaded on a worker's local memory}.

Clusters already use the efficient SLURM scheduler in order todistribute jobs.
\textbf{Thus our goal is to add a scheduler before the SLURM scheduler that would allocate to each job a node.}

\section{Framework}

SLURM~\cite{SLURM} is a cluster resource management system, flexible, fault-tolerant and highly scalable.
Schedulers are part of the standart configuration. The Backfill algorithm is very widely used~\cite{New_Backfill}.
Backfill is based on the First Come - First Served principle. While the scheduler is running, jobs in the queue are sorted by priority and queueing time~\cite{New_Backfill}.
If there are not enough nodes to start a job, Backfill takes the next job from the queue and sets it for execution if there are enough nodes for this job and 
it does not delay other jobs.
This algorithm allows to increase the density of supercomputer ressources' use by 20\% as well as reducing the average waiting time
for execution~\cite{Maui_Scheduler}.
Thus our starting point can be the Backfill algorithm.
\textbf{We will add a data-aware strategy that maximize data reuse while maintaining the Backfill's basic principle.}

\section{Problem Modeling}
We consider the problem of scheduling independent jobs $J$ on $K$ nodes,
denoted by $\Node{1},\ldots, \Node{K}$. \todo[inline]{Maxime: Should we talk about nodes or workers ?}
%~ As proposed in
%~ previous work~\cite{tasks-sharing-files07}, tasks sharing their input
%~ data can be modeled as a bipartite graph
%~ $G=(\taskset\cup\dataset, E)$. The vertices of this graph are on one
%~ side the tasks $\taskset=\{T_1,\ldots, T_m\}$ and on the other side
%~ the data $\dataset=\{D_1,\ldots, D_n\}$. An edge connects a task $T_i$
%~ and a data $D_j$ if task $T_i$ requires $D_j$ as input data. 
We denote by $\fileset$ the set of input files and $\jobset$ the set of jobs.
We denote by $\inputs(J_i)$ the set of input files required by job $J_i$. 
Each file has a size noted $\size(F_i)$.
Each of the $K$ nodes is equipped with a memory of limited size $M$. \todo{Maxime: Does all nodes have the same size ?}
%~ which may contain at most $M$ data simultaneously. 
Initially, all input files are stored in the main shared file system.
During the processing of a job $J_i$ on $\Node{k}$, all its inputs
$\inputs(J_i)$ must be in the memory of $\Node{k}$. 
We do not consider the data output of jobs.
Each of the $m$ jobs must be processed on some of the $K$ nodes. 
Our goal is to determine how to distribute the job set to
the nodes in order to minimize the total makespan.
Our objective is also to come up with a schedule with
few data movement, as they largely impact the overall processing
time as mentioned earlier. 

The file sharing system initially contains all of $\fileset$.
Each node is connected to the file sharing system with a limited bandwidth.
\todo[inline]{Maxime: Each node has a different bandwidth to load files ?}
The bounded bandwidth as well as the sizes of the files are the reasons why
we aim at restricting the amount of data movement.

We now define more formally the allocation of the tasks to the GPU and
their schedule.
We denote by $\sigma(k,i)$ the $i^\text{th}$ job
processed on $\Node{k}$ and by $\evict(k,i)$ the set of file to
be evicted from the memory of $\Node{k}$ before the processing
of this $i^\text{th}$ job.
\todo[inline]{Maxime: Can we control the eviction policy of files ?}

%~ We also let $\mathit{nb}_k$ be the number of tasks
%~ allocated to $\GPU{k}$. On $\GPU{k}$, the schedule is made of a
%~ succession of $\mathit{nb}_k$ steps, each step being composed of the
%~ following three stages (in this order):
%~ \begin{compactenum}
%~ \item All data in $\evict(k,i)$ are evicted (unloaded) from the
  %~ memory of $\GPU{k}$;
%~ \item The input data in  $\inputs(T_{\sigma(k,i)})$ that are not yet in
  %~ memory are loaded in the memory of $\GPU{k}$;
%~ \item Task $T_{\sigma(k,i)}$ is processed on $\GPU{k}$.
%~ \end{compactenum}

%~ Using the previous definition, we define the \emph{live data} $\live(k,i)$
%~ as the data in memory of $\GPU{k}$ during the computation of $T_{\sigma(k,i)}$, which
%~ can be defined recursively:
%~ $$
%~ \live(k,i)=
%~ \begin{cases}
  %~ \inputs(T_{\sigma(k,1)}) & \text{if~}i=1\\
%~ \Big(\live(k,i-1) \backslash \evict(k,i)\Big) \cup \inputs(T_{\sigma(k,i)})
%~ & \text{otherwise}
%~ \end{cases}
%~ $$
 %~ This is explained as follows: when processing its very first task, the
%~ memory of a GPU contains only the inputs of this task. When processing
%~ some task $T_j=T_{\sigma(k,i)}$ at step $i$, first some data are 
%~ possibly evicted from the memory of $\GPU{k}$ (that is
%~ $\evict(k,i)$), then the missing inputs of task $T_j$ are loaded.
%~ As noted above, each GPU has a bounded memory so it can only
%~ accommodate $M$ distinct input data (we recall that all data have the
%~ same size). This can be expressed as
%~ $$\forall k=1,\ldots,K, \forall i=1,\ldots,\mathit{nb}_k, \qquad \card{\live(k,i)}\leq M.$$
%~ Our objective is both to ensure a good load balancing and to minimize
%~ the amount of data movement:
%~ \begin{itemize}
%~ \item[\bf Objective 1: Load Balancing.] We assume that all tasks have the
  %~ same processing time on any GPU. Thus, load-balancing the work on
  %~ each GPU amounts to minimizing the maximum number of tasks on any GPU:
  %~ $$\textbf{Obj. 1}: \quad \mathit{minimize} \max_{k}  \mathit{nb}_k$$
%~ \item[\bf Objective 2: Data Movement.] The second objective is to limit
  %~ the amount of data movement, that is, to minimize the number of
  %~ \emph{load} operations from the main memory to the memory of the
  %~ GPUs: we consider that data are not modified so no \emph{store}
  %~ operation occurs when evicting a data from a GPU memory. With the
  %~ natural assumption that no input for task $T_{\sigma(k,i)}$ is
  %~ evicted from $\GPU{k}$ right before its processing
  %~ ($\evict(k,i) \cap \inputs(T_{\sigma(k,i)}) = \emptyset$), the
  %~ number of loads on $\GPU{k}$ can be computed as follows:
%~ $$
%~ \nbloads_k =\sum_i \Big|\inputs\left(T_{\sigma(k,i)}\right) \backslash \live(k,i-1)\Big|
%~ $$
%~ Then, our objective is simply to minimize the total number of loads:
  %~ $$\textbf{Obj. 2}: \quad  \mathit{minimize} \sum_k \nbloads_k$$
%~ \end{itemize}
%~ In prior work, the special case with a single GPU have been studied~\cite{gonthier:hal-03290998},
%~ and it has been shown that given a schedule $\sigma$, it is
%~ possible to derive an optimal eviction policy $\evict$ by following Belady's
%~ rule~\cite{belady66}: always evict the data whose next usage is the
%~ furthest in the future. This rule can be extended to the multi-GPU
%~ case: once tasks have been partitioned among GPUs and ordered for
%~ computation, that is, once $\sigma$ is set, we may compute the optimal
%~ eviction scheme for each GPU by applying Belady's rule. Hence our
%~ objective is only to find a schedule $\sigma$ of the tasks on
%~ the GPUs. The decision version of the bi-objective problem is then
%~ expressed as follows.
%~ \begin{definition}[Bi-Obj-Multi-GPU-Task-Scheduling]
  %~ Given a number $K$ of GPUs, $m$ tasks sharing their inputs according
  %~ to a bipartite graph $G$, and two bounds $W$ and $C$, is there a
  %~ schedule $\sigma$ % and an eviction policy $\evict$
  %~ such that
  %~ $\max_{k}  \mathit{nb}_k \leq W$ and $\sum_k \nbloads_k \leq C$?
%~ \end{definition}
%~ A previous study of the single-GPU
%~ case~\cite{gonthier:hal-03290998} proved that ordering tasks to
%~ minimize the data movement is NP-complete. As this sub-problem is
%~ contained in the more general problem presented here, this proves the
%~ complexity of the bi-objective problem.
%~ \begin{theorem}
  %~ The Bi-Obj-Multi-GPU-Task-Scheduling problem is NP-complete.
%~ \end{theorem}
%~ Note that the previous model can easily be extended to
  %~ heterogeneous tasks (with different processing times) and
  %~ heterogeneous data (with different sizes).
  
  
  
Many of the jobs will run on one core, so there is locality both across one node and connected with one worker thread. Does that matter or not?
If the access to the data is done through mmap it is actually possible to use the same data instance across threads within one node (NUMA effects can be considered) as well as for jobs running on the same thread. This also means that each job needs less memory, which could be important.
A question is how to think about load balancing, which also depends a bit on how to think about jobs. In general, the job queue will never run out, and probably there will be enough single core jobs to supply all worker threads (I think). But, there could also be the case where the user herself packages a lot of small work into one large job and then runs this on a full node. Then internal scheduling for load-balancing would be relevant, while all the small work would use the same reference data.

So you would have data locality on a thread and on a node.

With SLURM I can get info on idling nodes, dead nodes and occupied nodes (and their completion time). And I can get the info of a job (time it will take and space it will occupy and nodes required)
I need to co exist with other jobs using the mem map
Do we have user smart (giving good task sharing data) ?
All jobs would be all single nodes
We neglect jobs that use multiple nodes

En simu je peux faire des schedulers et en réel tester des choses plus modestes
Issue of core time a cause de petits jobs et de gros jobs qui arrive ? Un peu éloignées. Small job are schedule agressivly.	

We can use dynamic job to simulate transfers
Some jobs use memory map, writing files on the memory. An other job on the same node can read from this without writing himself beforehand. So if I am co-existing with other jobs using the same file, the memory space taken is smaller.
Should we go for a node even if we can't fill it completly with our jobs ? What do we loose from this ?
Do we have smart users ? Or do they just submit random jobs ? The case would be one user submiting 1000 jobs and then we give him advice on how to submit for it to perform in a good way. We could say the schedulers does everything or the schedulers give advices to the user on how to submit. Anyway the user will have to declare with flags dependencies and data. We can simulate 1000 jobs with the same file.
Sockets architecture can be a refinement.
All jobs are exclusively single nodes. It's rare to have multi-nodes and data shares and dependencies. We can translate this thus into a packing problem with jobs on one side and nodes on the other.
We need to schedule also with other jobs not using files.

On va partir de backfill

$m$ jobs.

\section{Measurements and experiments}

As mentioned before, the main measure will be the total makespan of a set of jobs.
However, other measures are also important:
\begin{itemize}
	\item	The number of core-hours used. With $NCore_i$ the number of processor units used by job $i$ and $T(j_i)$ the duration of job $i$ in hours, we have:
		$$
			\#Core-hours = \sum^{m}_{0} NCore_i \times T(j_i) + D(j_i) \notin M()
		$$
	\item	
\end{itemize}


Tradeoff in fairness (for users) and optimization ?

Load balancing and file re-use can be two opposing goals, so we would like to measure the impact on the performance depending on the strategy (Focus on load balancing or focus on file re-use or both for example).
cor hours
equity / queue time
total time
Tradeoff in fairness and optimization ? Yes we can do that
we can have nfo on nodes memory and file sizes
Maybe have data on which files are often used ?
measure core-hours were used + total time + en option queue time + Quantité de IO

Measure queue time ? Yes we can


\section{Data mining for an accurate simulation}
Estimated time of a transfer ? Can we simply compute it with the size of the input file and the bandwidth ? yes + dispersion/ecart type
I can data mine the jobs and the cluster information first, do a state of the art on batch scheduling with file sharing.

xml
json
Do we have a knowledge on the memory size allocated to each thread ? If yes we can compute the memory load on each worker and 
schedule accordingly.

We can get info on nodes memory (128 GB (m"moire disque) usually and 20 cores) and size of files (20 to a few hundreds GB).
Get statistics on which files are more popular ? We can extract metadata to get actual statistics. Having artificial set of files is fine.

\printbibliography
\end{document}
