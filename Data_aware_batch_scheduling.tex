\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx, amssymb, amsfonts, color, todonotes, diagbox, colortbl, pdfpages, listings, amsmath, caption, subcaption, xspace, xcolor, pifont, fullpage, algorithm, algpseudocode}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\setlength{\parskip}{0.2 cm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}

% Commands
\newcommand{\Node}[1]{\ensuremath{\mathrm{Node}_{#1}}\xspace}
\newcommand{\inputs}{\ensuremath{\mathcal{F}}\xspace}
\newcommand{\memory}{\ensuremath{\mathcal{M}}\xspace}
\newcommand{\duration}{\mathit{Duration}\xspace}
\newcommand{\core}{\mathit{Cores}\xspace}
\newcommand{\submissiontime}{\mathit{Subtime}\xspace}
\newcommand{\start}{\mathit{Starttime}\xspace}
\newcommand{\fileset}{\ensuremath{\mathbb{F}}\xspace}
\newcommand{\jobset}{\ensuremath{\mathbb{J}}\xspace}
\newcommand{\evict}{\ensuremath{\mathcal{V}}\xspace}
\newcommand{\nbloads}{\ensuremath{\mathit{\mathit{Loads}}}\xspace}
\newcommand{\live}{\ensuremath{L}\xspace}

\title{Data Aware Batch Scheduling}
\author{Maxime GONTHIER - Carl NETTELBLAD - Elisabeth LARSSON \\ Samuel THIBAULT - Loris MARCHAL}
\usepackage[backend=bibtex]{biblatex}
\bibliography{ref}

\begin{document}

\maketitle
\tableofcontents
\listoffigures
\newpage

\todo[inline]{Maxime: You can add highlighted comments this way: "\textbackslash todo[inline]\{Text\}".}
\todo{Maxime: Or with: "\textbackslash todo\\\{Text\}".}

\section{Motivation}

When using a HPC cluster, users may submits tens to hundreds jobs using the same multi-GB input file.
Users tend to submit each such run as a separate batch job.
The job will read inputs directly from a shared file system, which can be disastrous if the number of I/O are important.
We can thus ask ourselves, \textbf{how can we minimize the amount of transfers between the shared file system and the nodes ?}

Users can manually group together input files into a single job to reduce the effects from I/O reads.
Ideally, we would like to let the user submit jobs the way they want and schedule efficiently jobs on nodes.
The workers have a way to store files (with a limit on the memory size), as well as a cache page which contains the files/data recently used.
Thus, a way to minimize data transfers is to create a scheduler that \textbf{take into consideration data locality and schedule on the same node jobs sharing inputs}.

Our scheduler would be used to \textbf{distribute the load between the workers but also to have a vision of what the memory of each worker contains in order to re-use as much as possible a file already loaded on a worker's local memory}.

Clusters already use the efficient SLURM scheduler in order todistribute jobs.
\textbf{Thus our goal is to add a scheduler before the SLURM scheduler that would allocate to each job a node.}

\section{Related Work}

SLURM~\cite{SLURM} is a cluster resource management system, flexible, fault-tolerant and highly scalable.
Schedulers are part of the standard configuration. The Backfill algorithm is very widely used~\cite{New_Backfill}.
Backfill is based on the First Come - First Served principle. While the scheduler is running, jobs in the queue are sorted by priority and queueing time~\cite{New_Backfill}.
If there are not enough nodes to start a job, Backfill takes the next job from the queue and sets it for execution if there are enough nodes for this job and 
it does not delay other jobs.
This algorithm allows to increase the density of supercomputer ressources' use by 20\% as well as reducing the average waiting time
for execution~\cite{Maui_Scheduler}.
Thus our starting point can be the Backfill algorithm.
\textbf{We will add a data-aware strategy that maximize data reuse while maintaining Backfill's basic principle.}

\section{Problem Modeling}
We consider the problem of scheduling independent jobs $J$ on $K$ nodes,
denoted by $\Node{1},\ldots, \Node{K}$.
We denote by $\fileset$ the set of input files and $\jobset$ the set of jobs.
We denote by $\inputs(J_i)$ the set of input files required by job $J_i$. $\inputs(J_i)$ can be empty.
Each file has a size noted $\memory(F_i)$.
Each job has a submission time $\submissiontime(J_i)$ and an 
expected duration time $\duration(J_i)$.
Each job request a certain number of cores $\core(J_i)$. 
\todo[inline]{Maxime: Does all nodes have the same number of cores ? Can we have situation
where a job is requesting more cores than some nodes have ?}
Each of the $K$ nodes is equipped with a memory of limited size $\memory(\Node{i})$.
\todo[inline]{Maxime: Does all nodes have the same size ?}
Initially, all input files are stored in the main shared file system.
During the processing of a job $J_i$ on $\Node{k}$, all its inputs
$\inputs(J_i)$ must be in the memory of $\Node{k}$. 
We do not consider the data output of jobs.
Each of the $m$ jobs must be processed on some of the $K$ nodes. 
We consider that all jobs are single nodes.
Our goal is to determine how to distribute the job set to
the nodes in order to minimize the total makespan.
Our objective is also to come up with a schedule with
few data movement, as they largely impact the overall processing
time as mentioned earlier. 

The file sharing system initially contains all of $\fileset$.
Each node is connected to the file sharing system with a limited bandwidth.
\todo[inline]{Maxime: Each node has a different bandwidth to load files ?}
The bounded bandwidth as well as the sizes of the files are the reasons why
we aim at restricting the amount of data movement.

\todo[inline]{Maxime: It would make sense to represent our problem with a bi-partite graph were edges are file shares, nodes are jobs and the nodes are weighted with the duration of a jobs ? The issue is that I don't know how to represent the submission time ?}

There is locality both across one node and connected with one worker thread:
indeed some jobs use memory map. An other job on the same node can read from this without writing himself beforehand. 
So if a job is co-existing with other jobs using the same file, the memory space taken is smaller.
\todo[inline]{Maxime: Should we add a new layer of locality with for example: $\mathcal{M}(W_i)$, with $W_i$ one worker thread ?}

We now define more formally the allocation of the jobs to the node and
their schedule.
We denote by $\sigma(k,i)$ the $i^\text{th}$ job
processed on $\Node{k}$ and by $\evict(k,i)$ the set of file to
be evicted from the memory of $\Node{k}$ before the processing
of this $i^\text{th}$ job ($\evict(k,i)$ can be empty if there is enough space to fit the new files).

On $\Node{k}$, the schedule is made of a
succession of $\mathit{nb}_k$ (the number of jobs allocated) steps, each step being composed of the
following stages (in this order):
\begin{enumerate}
\item Eviction of files if the memory is saturated; \todo[inline]{Maxime: Can we control the eviction policy of files ?}
\item The input files in $\inputs(J_{\sigma(k,i)})$ that are not yet in memory are loaded in the memory of $\Node{k}$;
\item Job $J_{\sigma(k,i)}$ is processed on $\Node{k}$ during at most $\duration(J_i)$.
\end{enumerate}

We define the \emph{live data} $\live(k,i)$
as the files in memory of $\Node{k}$ during the computation of $J_{\sigma(k,i)}$, which
can be defined recursively:
$$
\live(k,i)=
\begin{cases}
  \inputs(J_{\sigma(k,1)}) & \text{if~}i=1\\
\Big(\live(k,i-1) \backslash \evict(k,i)\Big) \cup \inputs(J_{\sigma(k,i)})
& \text{otherwise}
\end{cases}
$$

Let's denote $\mathit{Makespan}$, the time it took to execute $\jobset$ on a set of $K$ nodes.

Our objective is double:
\begin{itemize}
	\item \textbf{Objective 1: Minimize makespan}
		$$
			\textbf{Obj. 1}: \quad \mathit{minimize}~\mathit{Makespan}
		$$
	\item \textbf{Objective 2: Minimize files movement} 
		The second objective is to limit
	    the amount of file movement, that is, to minimize the sum of the sizes of
	    each file loaded from the file sharing system to the memory of the
	    nodes. We consider that files are not modified. With the
	    natural assumption that no input for job $J_{\sigma(k,i)}$ is
	    evicted from $\Node{k}$ right before its processing
	    ($\evict(k,i) \cap \inputs(J_{\sigma(k,i)}) = \emptyset$), the
	    amount of loads $\memory$ on $\Node{k}$ can be computed as follows:
		$$
			\nbloads_k =\sum_i \memory(\inputs\left(J_{\sigma(k,i)}\right) \backslash \live(k,i-1))
		$$
		Then, our objective is simply to minimize the total number of loads:
		$$
			\textbf{Obj. 2}: \quad  \mathit{minimize} \sum_k \nbloads_k
		$$
\end{itemize}

\begin{definition}[Bi-Obj-Batch-Scheduling]
  Given a number $K$ of nodes, $m$ jobs sharing their inputs files
  %~ according to a bipartite graph $G$, 
  and two bounds $W$ and $C$, is there a
  schedule $\sigma$ % and an eviction policy $\evict$
  such that $\mathit{Makespan} \leq W$ and $\sum_k \nbloads_k \leq C$?
\end{definition}

Load balancing/minimizing makespan and file re-use can be two opposing goals, so we would like to measure the impact on the performance depending on the strategy (Focus on load balancing or focus on file re-use or both for example) ?

\section{Other measurements}

As mentioned before, the main measure will be the total makespan of a set of jobs as well as the amount of loads.
However, other measures are also important:

Each job has a start time $\start(J_i)$, the actual time at which the job was executed.
We also want to measure the average queue time of a job in order
to evaluate our scheduler in terms of fairness. We can do:
$$
	Average\_queue\_time = \frac{\sum^{m}_{i = 0} \start(J_i) - \submissiontime(J_i)}{m}
$$
\todo[inline]{Maxime: Should this be in the objectives above ?}

We also want to measure the core-hours used. We need to take into consideration the amount of files loaded before the computation.
With $\core(J_i)$ the number of processor units used by job $i$ and $\duration(J_i)$ the duration of job $i$ in hours, we have:
$$
	\#Corehours = \sum^{m}_{i = 0} \core(J_i) \times (\duration(J_i) + \memory(\inputs\left(J_{\sigma(k,i)}\right) \backslash \live(k,i-1)))
$$

\section{The use of simulation}

The use of simulation is motivated both by the fidelity of simulated results as well as energy savings. 
We will be using Batsim, a precise and realistic~\cite{Batsim} resource and job management system (RJMS) simulator.
Batsim JSON-formatted workloads are extensible and allow easy workload generation. 
Moreover, Batsim provides clear information about scheduling metrics and job placement.
We will add a scheduler using Batsched~\footnote{\url{https://gitlab.inria.fr/batsim/batsched}}.

\paragraph{Data mining for an accurate simulation}
A number of informations must be retrieved in order to accurately 
simulate the use case of Uppsala's University cluster:
\begin{itemize}
	\item	.xml files of the cluster containing informations on the nodes.
	\item	Bandwidth of each node in order to compute the estimated time of a file load with: $\frac{\memory(F_i)}{\mathit{Bandwidth(\Node{i})}}$.
	\item	Size of input files (20 to 100 GB ?)
	\item	Duration of jobs
	\item	Are some files more popular ?
	\item	Total number of jobs
	\item	Number of available nodes
	\item	Size of node's memory (128 GB ?)
\end{itemize}

\paragraph{How to simulate data transfers ?}
We can add dynamic job of duration the time it would take to transfer the file on the node in order
to simulate file loads.

\printbibliography
\end{document}
