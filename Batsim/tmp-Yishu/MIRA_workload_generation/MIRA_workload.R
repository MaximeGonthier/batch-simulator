#!/usr/bin/env
rm(list=ls())
#setwd("/home/yishu/Desktop")
#library(battools)
library(dplyr)
library(curl)

assert_in = function(x, set) {
  assertthat::assert_that(x %in% set,
                          msg=paste(deparse(substitute(x)), "is not in", sprintf("{%s}", paste(set, collapse=","))))
}

`%>%` <- magrittr::`%>%`

#' Write a Batsim workload to a file.
#'
#' @param x The Batsim workload to be written.
#' @param path Path to write to.
#' @export
write_batworkload = function(x, path) {
  cat(x, file=path)
}

#' Transform a SWF trace into a Batsim workload with delay profile types
#'
#' This function generates a Batsim input workload from a SWF trace.
#' The job profiles will use the "delay" profile type.
#'
#' @param swf A SWF tibble, as generated by read_swf
#' @param nb_res The number of resources in the workload. Loaded from SWF comments if value is "swf", otherwise assuming an integer value.
#' @param subtime_strat The submission time generation technique.
#'                      If "swf" is given, values are kept from swf.
#'                      If "translate_to_zero" is given, values are kept from swf but translated by the same amount so that the first job is submitted at time=0
#' @param walltime_strat The walltime generation technique.
#'                       If "swf" is given, walltimes are kept from swf.
#' @param profile_strat The profile generation technique.
#''                     If "one_per_similar_job" is given, jobs can share profiles if their execution time is close to each other.
#'                      If "one_per_job" is given, one profile is created for each job.
#' @param similarity_threshold The similarity threshold used for the "one_profile_per_similar_job" profile_strat.
#'                             This is an integral value in seconds that represent the time granularity to use so that similar jobs are grouped together.
#'                             Jobs are rounded up to match a multiple of this value ; this means the minimal execution time will be similarity_threshold.
#' @return A Batsim workload (JSON representation)
#' @export
swf_to_batworkload_delay = function(swf, nb_res="swf", subtime_strat="swf", walltime_strat="swf", profile_strat="one_per_similar_job", similarity_threshold=1) {
  assertthat::assert_that(nb_res %in% c("swf") || (is.atomic(nb_res) && is.numeric(nb_res) && nb_res == as.integer(nb_res)),
                          msg='nb_res must be in {"swf"} or given as an integral number')
  assertthat::assert_that(is.numeric(nb_res), msg="parsing nb_res from swf comments is unimplemented. please make it explicit in your function call.")
  assert_in(subtime_strat, c("swf", "translate_to_zero"))
  assert_in(walltime_strat, c("swf"))
  assert_in(profile_strat, c("one_per_job", "one_per_similar_job"))
  assertthat::assert_that(is.atomic(similarity_threshold) && is.numeric(similarity_threshold) && similarity_threshold == as.integer(similarity_threshold) && similarity_threshold >= 1, msg="similarity_threshold must be an integral, strictly positive number")
  
  # Sanity setups to make sure everything goes well.
  swf_sane = swf %>% dplyr::mutate(
    # jobs failed if marked as such or negative run_time
    return_code = ifelse(status == 1 && run_time >= 0, 0, 1)
  ) %>% dplyr::mutate(
    # infinite walltimes are -1 in batsim input workloads
    requested_time = ifelse(requested_time <= 0, -1, requested_time),
    # make sure simulation inputs are valid (no time travel/negative execution times)
    run_time = ifelse(run_time < 1, 1, run_time)
  )
  
  # Shape data in easy-to-handle data structures.
  jobs = swf_sane %>% dplyr::transmute(
    id = as.character(job_number),
    subtime = submit_time,
    walltime = requested_time,
    res = requested_number_of_processors,
    runtime = run_time
  )
  if (subtime_strat == "translate_to_zero") {
    minimum_subtime = min(swf_sane[["submit_time"]])
    jobs = jobs %>% dplyr::mutate(subtime = subtime - minimum_subtime)
  }
  
  if (profile_strat == "one_per_job") {
    # Each job has its dedicated profile.
    jobs = jobs %>% dplyr::mutate(profile = id)
    profiles = swf_sane %>% dplyr::transmute(
      name = as.character(job_number),
      delay = run_time,
      ret = return_code
    )
  } else { # "one_per_similar_job"
    jobs = jobs %>% dplyr::mutate(profile = ceiling(runtime / similarity_threshold) * similarity_threshold)
    profiles = swf_sane %>% dplyr::transmute(
      name = as.character(ceiling(run_time / similarity_threshold) * similarity_threshold),
      delay = ceiling(run_time / similarity_threshold) * similarity_threshold,
      ret = return_code
    ) %>% dplyr::distinct()
  }
  
  jobs_json = jobs %>% dplyr::transmute(
    repr = sprintf('{"id":"%s",
                     "subtime":%f,
                     "walltime":%g,
                     "res":%d,
                     "profile":"%s",
                     "priority":%d}', id, subtime, walltime, res, profile, 0) #res/the normalized constant
  )
  
  profiles_json = profiles %>% dplyr::transmute(
    repr = sprintf('"%s":{"type":"delay",
                          "delay":%g,
                          "checkpoint_period":%g, 
                          "checkpoint_cost":%g, 
                          "recovery_cost":%g, 
                          "Tfirst":%g, 
                          "Rfirst":%g,
                          "total_failure_rate":%g}', name, delay, 0.0, 300, 300, 0.0, 0.0,total_failure_rate_tobewriten) #checkpoint period
  )
  
  sprintf('{"nb_res":%d,"jobs":[%s],"profiles":{%s}}', nb_res,
          paste(jobs_json[["repr"]], collapse=","),
          paste(profiles_json[["repr"]], collapse=",")
  )
}


# MIRA 2017
# mydata = read.csv("./job_trace_2017.csv")

# MIRA 2018
# mydata = read.csv("/home/yishu/Desktop/job_trace_2018.csv")
mydata = read.csv("./job_trace_2018.csv")

mydata$submit_time=as.numeric(as.POSIXct(mydata$QueuedTimestamp))
mydata=mydata[order(mydata$submit_time),]
mydata$job_number <- seq.int(nrow(mydata))
colnames(mydata)[which(names(mydata) =="Runtime")] <-"run_time"
colnames(mydata)[which(names(mydata) =="X.NodesRequested")] <-"requested_number_of_processors"
colnames(mydata)[which(names(mydata) =="WallTimeRequested")] <-"requested_time"

mydata = mydata[,c(-1,-2,-3,-5,-8,-9,-10,-12,-13)]
mydata$wait_time=1
mydata$number_of_allocated_processors=1
mydata$average_cpu_time_used=1.00
mydata$used_memory=-1
mydata$requested_memory=-1
mydata$status=1
mydata$user_id=1
mydata$group_id=1
mydata$application_number=1
mydata$queue_number=1
mydata$partition_number=-1
mydata$preceding_job_number=-1
mydata$think_time_from_preceding_job=-1

# #June 2017
# mydata6 =  mydata %>% filter(mydata$job_number>=28970 & mydata$job_number<=32280)
# sum(mydata6$requested_number_of_processors/100*mydata6$run_time*100)/(49152*30*24*3600)
# month = mydata6

# March 2018
mydata3 =  mydata %>% filter(mydata[,6]>=8661 & mydata[,6]<=12684)
sum(mydata3$requested_number_of_processors/100*mydata3$run_time*100)/(49152*30*24*3600)
month = mydata3

# if walltime is smaller than runtime, then we change walltime to runtime + 60
month[month$requested_time<=month$run_time,]$requested_time = month[month$requested_time<=month$run_time,]$run_time + 60

# modify the largest-size 49152 job which can not run in a failure-prone platform to job-size 49000
# month = month[month$requested_number_of_processors!=49152, ]
month[month$requested_number_of_processors==49152, ]$requested_number_of_processors = 49000


# # 1. This is the original workload without checkpoint
# total_failure_rate_tobewriten = 0

# 2. This is considering the checkpoint cost, we should add the checkpoint cost according to MTBF to runtime and walltime
checkpoint_time = 300
MTBF = 3600/3*2
total_failure_rate = 1/(MTBF*49152) #Here, total_failure_rate is for a single processor
checkpoint_period = sqrt((2*checkpoint_time)/(total_failure_rate*month$requested_number_of_processors))
month<-cbind(month,checkpoint_period)
a=floor(month$run_time/month$checkpoint_period)*checkpoint_time
#Here, the runtime contains the usefultime (original run time) and checkpoint cost
month$run_time = month$run_time + floor(month$run_time/month$checkpoint_period)*checkpoint_time

#Here, the corresponding walltime should also add the checkpoint cost
month$requested_time = month$requested_time + floor(month$requested_time/month$checkpoint_period)*checkpoint_time

total_failure_rate_tobewriten = 1/(MTBF*49152)*10^8 # because in c++, it will consider 10^(-9) as 0, thus here we times 10^8 and then in c++ divided by 10^8
#Now, you can run the generate-workload.R to generate the MIRA workload

# Generate a Batsim workload.
workload = swf_to_batworkload_delay(month, 49152, subtime_strat = "translate_to_zero")
write_batworkload(workload, "./MIRA_2018March_MTBF40min.json")
