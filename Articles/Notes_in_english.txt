From file tuning_easybf.pdf : "Tuning EASY-Backfilling Queues"
	Goal: Tuning EASY-bf using queue reordering policies in order
	to minimize the average waiting time
	Describe the EASY-bf algorithm: The EASY heuristic uses a job queue to perform job starting/reservation
	(the primary queue) and job backfilling (the backfilling queue). These queues
	can be dissociated and the heuristic can be parametrized via both a primary
	policy and a backfilling policy. This is typically done by ordering both queues in
	an identical manner using job attributes.
	Add a pseudo-code of EASY-bf
	Will test 7 different queue reordering policies: FCFS: First-Come First-Serve, which is the widely used default policy [24].
	LCFS: Last-Come First-Serve.	LPF: Longest estimated Processing time p e j First.	SPF: Smallest estimated Processing time p e j First [25].
	LQF: Largest resource requirement q j First.	SQF: Smallest resource requirement q j First.
	Conclusion: The first conclusion is that reordering the primary queue is more
	beneficial than simply reordering the backfilling queue. However, this introduces
	a risk on the maximum values of the objective, which we control by hybridiz-
	ing FCFS and the reordering policy via a thresholding mechanism. Finally, we
	showed that the experimental performance of the thresholded heuristics general-
	izes well. Therefore, this framework allows a system administrator to tune EASY
	using a simulator. Moreover, the attitude torwards risk in maximum values can
	be adapted via the threshold value. With a low threshold value, the increase
	in maximal cost is small but the learned policy does not take too much risk.
	It is possible to gain more by increasing the threshold, but this comes with an
	increase in the maximal cost. Two questions concerning the learning of EASY
	policies arise from this work.
	First, the stability of other EASY heuristic classes remains unknown. The
	”simple” class of composed of 7 primary policies and 7 backfilling policies (cardi-
	nality 49) can generalize using thresholding.

From file ccgrid.2002.1017108.pdf : "Adaptive Scheduling under Memory Pressure on Multiprogrammed Clusters"
	Present an adaptive scheduling
	strategy that prevents thrashing, enables adaptation to
	memory pressure at arbitrary points of execution in a
	program, and takes into account the relative priorities
	of jobs.

Stefanos's papers:
	"Cache Replacement Based on Reuse-Distance Prediction". Try to predict reuse distance and use this information for eviction. The main idea is that the future is not know but you can still predict it to make decision, just like in our case with tasks and data. Here reuse distance is two consecutive access of the smae cacheline. But can be extended to data for us. Use real information and a prediction with a confidence. As the real information is updated, so is the confidence of the prediction (decreased or increased). Then discussion very specific to caches sizes. To know what to evict: "We look for the cacheline that is going to be accessed farthest in the future by computing its Estimated Time of Access (ETA). The ETA of a line is the time it was last accessed plus its predicted reuse-distance minus the current time. In other words, a cacheline’s ETA is the (predicted) number of accesses that separate the present moment from its next access". In the case they have no prediction they just evict cacheline whose predicted reuse time has passed. Once a cacheline was found the fiursthers in the future, they won't necessarly evict it cause you can find even better sometimes. "The longer a line remains unaccessed the higher the probability that it is useless". We now have two candidates, quantified by exactly the same metric: time measured in accesses. One candidate is the line with the largest ETA (the ETA line), and the other is the line with the largest Decay time (the LRU line). We pick the largest of the two for replacement. IN OUR CASE: We want the workload to be unknown and possibly not regular ? So we would need to estimate the number of time a data will be used to get the next reuse time. Or we could look at the frequency of usage of a data in the past to predict the future. So if a data is used a lot in short interval we can say that the next use is soon. Whereas if it's used only sparsely, we can evict it cause anyway it's not regular. + We can use our buffer information.
	But for us: We know enough of the future to not use this prediction. And our applications are too irregular to use this kind of prediction. We don't know if a data will be re used in some time because applications changes with time (because of dependencies for example). 
	By knowing the set of available task, we can already produce good eviction without guessing which data will be available later on.
