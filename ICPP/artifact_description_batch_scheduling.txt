Simple formatting can be achieved in the questions below using LaTeX. Blocks using LaTeX formatting must be wrapped in "%START_LATEX" and "%STOP_LATEX" comment lines. For example:
%START_LATEX
\begin{enumerate}
\item Your formatted text can go here.
\item Simple things like bold or italics will work.
\end{enumerate}
%STOP_LATEX

The Artifact Description is an optional step. Authors can provide descriptions of their artifacts. By filling in this form, the Artifact Description (AD) Appendix will be auto-generated from author responses to a standard form embedded in the online submission system. It must include the following aspects:


Artifact Identification

It includes:
(i) an abstract describing the main contributions of the article and the role of the computational artifact(s) in these contributions.
(ii) the abstract may include a software architecture or data models and its description to help the readers understand the computational artifact(s) and
(iii) a clear description on to what extent the computational artifact(s) contribute(s) to the reproducibility of the experiments in the article.

*** Our submission:

Clusters typically use workload schedulers, such as the Slurm workload
manager, to allocate computing jobs across nodes.  Such schedulers
seek a good compromise between increasing resource utilization and
user satisfaction, but they do not take into account jobs that share
large input files, which can happen in data-intensive scenarios.  In
this paper, we study how to design a data-aware job scheduler that can
take advantage of previously-loaded files to reduce data transfers and
thus reduce job wait times.  We present three new schedulers that are
able to distribute the load between the computing nodes as well as to
reuse the input files already loaded in the memory of some node and we
compare them with the most commonly-used strategies: FCFS and EFT.
Simulation results show that keeping data in local memory between
successive jobs and using data-locality information to schedule jobs
improves performance compared to FCFS in two ways: a reduction in job
waiting time and a reduction in the amount of data transfers. The
computational artifact consists in the simulator used to obtain these
results.

The data used by our simulator are extracted from real logs from a
university cluster shared by several research laboratories.  We
perform the experiments on 12 different workloads, each containing 7
days of logs. We simulate all studied schedulers on these logs, and
measure the stretch (time between the job submission and the job
completion, divided by the execution time of the job) of each user
session (jobs submitted by a user in a 5-minute window), as well as
the time spent moving data.

The computational artifacts contain everything needed to reproduce the
experiments exactly.  It includes the workloads used, our simulator,
the platform description, the schedulers, and the script used to
launch the experiments and draw the resulting figures.



Reproducibility of Experiments

It includes:
(i) a complete description of the experiment workflow that the computational artifact(s) can execute
(ii) an estimation of the execution time to execute the experiment workflow.
(iii) a complete description of the expected results and an evaluation of them, and most importantly.
(iv) how the expected results from the experiment workflow relate to the results found in the article. Best practices indicate that, to facilitate the understanding of the scope of the reproducibility, the expected results from the artifact should be in the same format as the ones in the article. For instance, when the results in the article are depicted in a graph figure, ideally, the execution of the code should provide a (similar) figure (there are open-source tools that can be used for that purpose such as gnuplot). It is critical that authors devote their efforts on these aspects of the reproducibility of experiments to minimize the time needed for their understanding and verification.

*** Our submission:

The computational artifact runs a simulation of the 12 evaluated
workloads using the batch scheduler simulator we implemented.  For
each week, a workload is associated in the form of a text file
containing the job's id, submission time, number of requested cores,
input file, expected duration, and actual duration.  The simulator
processes the workload using each scheduler.  The output is the
stretch of each user session.  After simulating the 12 weeks, a script
produces the results: boxplots showing the stretch improvement over
the baseline for each scheduler.  The expected completion time is
about 30 hours on a common laptop.  The expected results are Figures
2, 7, 9, and 11.  Since we are running a deterministic simulation, the
results should be an exact replica of these figures.
