2021-11-04 23:51:20 jobstate=COMPLETED jobid=22838152 username=srimag account=snic2021-22-167 start=1636066245 end=1636066280 submit=1636063769 nodes=r72 procs=6 batch=yes jobname=nf-NFCORE_RNASEQ_RNASEQ_SUBREAD_FEATURECOUNTS_(MG_M_RT_rep1) partition=core limit=08:00:00

2021-11-04 23:51:20 jobstate=COMPLETED jobid=22837935 username=srimag account=snic2021-22-167 start=1636066245 end=1636066280 submit=1636063399 nodes=r295 procs=6 batch=yes jobname=nf-NFCORE_RNASEQ_RNASEQ_RSEQC_RSEQC_BAMSTAT_(MG_M_RT_rep3) partition=core limit=08:00:00

2021-11-04 23:51:23 jobstate=COMPLETED jobid=22838258 username=cnluzon account=snic2020-15-9 start=1636066245 end=1636066283 submit=1636064124 nodes=r316 procs=8 batch=yes jobname=deeptools_fingerprint_1273 partition=core limit=18:00:00

2021-11-04 23:51:26 jobstate=COMPLETED jobid=22837181 username=louel account=snic2020-15-61 start=1636064925 end=1636066286 submit=1636061885 nodes=r267 procs=6 batch=yes jobname=nf-MACS2_(K4me3-AZA_R3_vs_input-AZA_R3) partition=core limit=08:00:00

2021-11-04 23:51:28 jobstate=COMPLETED jobid=22837808 username=srimag account=snic2021-22-167 start=1636065915 end=1636066288 submit=1636062979 nodes=r194 procs=6 batch=yes jobname=nf-NFCORE_RNASEQ_RNASEQ_BEDTOOLS_GENOMECOV_(MG_M_RT_rep2) partition=core limit=08:00:00

2021-11-04 23:51:29 jobstate=COMPLETED jobid=22839619 username=madeline account=p2018002 start=1636066245 end=1636066289 submit=1636065930 nodes=r382 procs=1 batch=yes jobname=snakejob.af_counts.3304.sh partition=core limit=1-00:00:00

2021-11-04 23:51:37 jobstate=COMPLETED jobid=22837749 username=louel account=snic2020-15-61 start=1636065585 end=1636066297 submit=1636062750 nodes=r350 procs=6 batch=yes jobname=nf-BIGWIG_(K4me3-COMBO_R2) partition=core limit=08:00:00

2021-11-04 23:51:38 jobstate=COMPLETED jobid=22837869 username=srimag account=snic2021-22-167 start=1636066245 end=1636066298 submit=1636063284 nodes=r47 procs=6 batch=yes jobname=nf-NFCORE_RNASEQ_RNASEQ_RSEQC_RSEQC_INNERDISTANCE_(MG_M_RT_rep3) partition=core limit=08:00:00

2021-11-04 23:51:39 jobstate=COMPLETED jobid=22837873 username=srimag account=snic2021-22-167 start=1636066245 end=1636066299 submit=1636063289 nodes=r58 procs=6 batch=yes jobname=nf-NFCORE_RNASEQ_RNASEQ_RSEQC_RSEQC_JUNCTIONSATURATION_(MG_M_RT_rep3) partition=core limit=08:00:00

2021-11-04 23:51:43 jobstate=COMPLETED jobid=22837899 username=srimag account=snic2021-22-167 start=1636066245 end=1636066303 submit=1636063319 nodes=r173 procs=6 batch=yes jobname=nf-NFCORE_RNASEQ_RNASEQ_STRINGTIE_(MG_M_RT_rep3) partition=core limit=08:00:00

2021-11-04 23:51:45 jobstate=COMPLETED jobid=22831033 username=maxil account=p2018003 start=1636061959 end=1636066304 submit=1636019669 nodes=r461 procs=2 batch=yes jobname=HC_HGDP00769_Japanese.13 partition=core limit=3-00:00:00

2021-11-04 23:51:51 jobstate=COMPLETED jobid=22839641 username=madeline account=p2018002 start=1636066245 end=1636066311 submit=1636065958 nodes=r241 procs=1 batch=yes jobname=snakejob.af_counts.1691.sh partition=core limit=1-00:00:00
----------------------------------------------------------------

One tool that has appeared many times is Kraken2 ( https://github.com/DerrickWood/kraken2/tree/master/src ). It can be used with reference DBs of widely different size, from 5 GB to 400 GB. It mmaps that reference file. About a year ago (around April 14 2021), the user name juliette was running 40 or so jobs, accessing the file /sw/data/Kraken2_data/latest_nt/hash.k2d. It was smaller then, around 200 GB. It’s currently 360 GB. A specific job ID from that period was 19414225.

A specific set of runs causing trouble in our file system due to the resulting small IO requests to central storage were around September 29 2021, from a user called micdon – all jobs from him with job ids starting in 223 and 224 (22[3]….. as a regexp on the ID string), 119 of them.

From September last year, we also had trouble with a tool called hal2maf. A specific set of runs causing trouble in our file system due to the resulting small IO requests to central storage were around September 29 2021, from a user called micdon – all jobs from him with job ids starting in 223 and 224 (22[3]….. as a regexp on the ID string), 119 of them. The file itself is /proj/uppstore2017228/KLT.04.200M/200m_MD/data/new_250_MAMMALS_v2_20201120/HAL/241-mammalian-2020v2.hal, 800GB (!) in size. However, each job was only supposed to access a subregion, and I’m not sure myself to what extent different runs actually accessed the same part of the file. In this sense, the Kraken jobs are a better representative for what we’re looking for.
----------------------------------------------------------------

module load uppmax
squeue
sacct --user=<username>

[maxim@rackham2 ~]$ jobstats -p -r 25756958 or finishedjobinfo -j 25757038 do I have the rights ?
*** snic2021-5-350 is not one of your projects (jobid 25756958)
*** 1 total jobs
*** 1 jobs running, 0 jobs not running
[maxim@rackham2 ~]$ 

look at the accounting history in files named e.g. /sw/share/slurm/rackham/accounting/2021-04-13, and find out job ids for the jobs involved ?
How do you get the files used ?

Get job after certain date of a certain user
sacct -S start-date -u user-name

[maxim@rackham2 ~]$ perl jobstats_maxime -p 25821517
Running '/sw/uppmax/bin/finishedjobinfo -M rackham -j 25821517' for more information, please be patient...
*** Jobid 25821517 not found
*** 1 total jobs
*** 0 jobs run, 1 jobs not run
What am I doing wrong ?

Info sur un job en cours
scontrol show jobid -dd job_id

Once your job has completed, you can get additional information that was not available during the run. This includes run time, memory used, etc.
To get statistics on completed jobs by jobID:
sacct -j <jobid> --format=JobID,JobName,MaxRSS,Elapsed

To view the same information for all jobs of a user:
sacct -u <username> --format=JobID,JobName,MaxRSS,Elapsed

25775951     msp_0.9_5+       core snic2021-+          5 OUT_OF_ME+    0:125 
25775951.ba+      batch            snic2021-+          5 OUT_OF_ME+    0:125 
25775951.ex+     extern            snic2021-+          5  COMPLETED      0:0 
[maxim@rackham2 ~]$  sacct -j 25775951 --format=JobID,JobName,MaxRSS,Elapsed
       JobID    JobName     MaxRSS    Elapsed 
------------ ---------- ---------- ---------- 
25775951     msp_0.3_5+              00:32:56 
25775951.ba+      batch  32123913K   00:32:56 
25775951.ex+     extern       114K   00:32:56 
[maxim@rackham2 ~]$ 

Pourquoi out of mem alors que ce n'est que 32GB ?

squeue -t PENDING ou COMPLETED

Info subtime etc
squeue --format="%.18i %.8u %.10M %.9l %.6D %V"  -t COMPLETED > log.txt
Puis
<pre>sacct -j 25131821 --format=JobID,JobName,MaxRSS,Elapsed
pour des infos sur la mémoire utilisé

90% des jobs ont peu de données (entre 60 et 600 KB)
Le reste peu avoir de 30 BG à plus de 1000 GB. Le temps de ces jobs est alors de 20h ou plus.
Pb: job de 5 secondes qui mette un walltime de 9h

Logs par date dans
/sw/share/slurm/rackham/accounting
